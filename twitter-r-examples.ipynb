{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo Twitterdaten mit `R`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt zahlreiche Tools, mit denen Forschende *Twitter*-Daten sammeln können. Die meisten von diesen nutzen die [Application Programming Interfaces (APIs) von Twitter](https://developer.twitter.com/en/docs/twitter-api) für den Zugriff auf die Daten, während andere Web. bzw. [Screen Scraping](https://de.wikipedia.org/wiki/Screen_Scraping) nutzen.\n",
    "\n",
    "Neben der Art des Datenzugriffs unterscheiden sich die zahlreichen Tools für die Arbeit mit *Twitter*-Daten auch noch auf vielen anderen Dimensionen, wie z.B.:\n",
    "- Verfügt das Tool über ein Graphical User Interface (GUI)?\n",
    "- Handelt es sich bei dem Tool um Free and Open Source Software (FOSS)?\n",
    "- Sind Programmierkenntnisse zur Nutzung nötig? Wenn ja, in welcher Programmiersprache?\n",
    "- Welche Art von Daten kann das Tool sammeln?\n",
    "- Kann das Tool nur für die Erhebung von *Twitter*-Daten genutzt werden oder auch für deren Aufbereitung und Auswertung?\n",
    "- Welche Version(en) der Twitter API wird/werden genutzt?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Auflistung und Diskussion aller Tools für die Arbeit mit *Twitter*-Daten würden den Rahmen dieser kurzen Demo deutlich sprengen, aber es gibt einige Übersichten (manche davon sind zu Social-Media-Daten allgemein, andere spezifisch zu Twitter):\n",
    "- [Twitter Tool List](https://smo-wiki.leibniz-hbi.de/Twitter-Tools) im [Wiki](https://smo-wiki.leibniz-hbi.de/) des [*Social Media Observatory*](https://smo.leibniz-hbi.de/) am [Leibniz-Institut für Medienforschung | Hans-Bredow-Institut (HBI)](https://hans-bredow-institut.de/en)\n",
    "- [Social Media Research Toolkit](https://socialmedialab.ca/apps/social-media-research-toolkit-2/) vom Social Media Lab der Ryerson University\n",
    "- [Blog-Post *Social media data in research*](https://ocean.sagepub.com/blog/social-media-data-in-research-a-review-of-the-current-landscape) von Lily Davies bei *Sage Ocean* \n",
    "- [Blog-Post *Using Twitter as a data source*](https://blogs.lse.ac.uk/impactofsocialsciences/2021/05/18/using-twitter-as-a-data-source-an-overview-of-social-media-research-tools-2021/?s=09) von Wasim Ahmed\n",
    "\n",
    "Zudem arbeiten wir im Team [*Research Data & Methods*](https://www.cais.nrw/cais/team/#research) am [Center for Advanced Internet Studies (CAIS)](https://www.cais.nrw/) derzeit an einer Guideline mit dem Titel *Collecting Social Media Data - Tools for obtaining data from Social Media Platforms*, die bald veröffentlicht werden soll."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie den o.g. Übersichten entnommen werden kann, gibt es neben Tools, die keine Programmierkenntnisse voraussetzen und über eine GUI verfügen (wie z.B. [*TAGS*](https://tags.hawksey.info/)), zahlreiche andere, die für bestimmte Programmiersprachen geschrieben wurden. Die meisten dieser Tools sind entweder `Python` Libraries (wie z.B. [`twarc`](https://twarc-project.readthedocs.io/en/latest/)) oder `R`-Pakete (wie z.B. [`academictwitteR`](https://github.com/cjbarrie/academictwitteR)). Darüber hinaus gibt es auch noch einige Multi-Platform-Tools, mit denen es möglich ist, *Twitter*-Daten zu sammeln (wie z.B. [facepager](https://github.com/strohne/Facepager/) oder [vosonSML](http://vosonlab.net/SocialMediaLab))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbeitsschritte & Datentypen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook arbeiten wir mit `R`. Der wesentliche Vorteil davon ist, dass wir mit `R` nicht nur *Twitter*-Daten erheben, sondern diese auch auf vielfältige Art aufbereiten und auswerten können. Zudem ist mit `R` prinzipiell alles \"customizable\": Von der Sammlung der Daten über die Pipeline zur Aufbereitung bis hin zur Visualisierung und Auswertung. Die Nutzung von `R` für die Arbeit mit *Twitter*-Daten ist voraussetzungsreicher, erlaubt uns aber maximale Flexibilität."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ur Sammlung der *Twitter*-Daten nutzen wir hier das Paket [`academictwitteR`](https://github.com/cjbarrie/academictwitteR). Wie so oft im `R`-Universum gibt es auch für die Sammlung, Aufbereitung und Analyse von *Twitter*-Daten verschiedene Pakete. Sehr umfangreiche Optionen für den Datenzugriff über die [*Twitter* API v1.1](https://developer.twitter.com/en/docs/twitter-api/v1) bieten etwa die Pakete [`rtweet`](https://docs.ropensci.org/rtweet/index.html) von [*rOpenSci*](https://ropensci.org/) und [`vosonSML`](https://vosonlab.github.io/vosonSML/) vom [*VOSON Lab*](http://vosonlab.net/). Eine Alternative zu `academictwitteR` für die Nutzung des [*Academic Product Track*](https://developer.twitter.com/en/products/twitter-api/academic-research) für die [*Twitter* API v2](https://developer.twitter.com/en/docs/twitter-api) ist das `R`-Paket [`einfach`](https://github.com/chainsawriot/einfach), welches auch eine GUI bietet (eine weiteres `R`-Paket für *Twitter*-Daten mit einer GUI ist [`VOSONdash`](https://vosonlab.github.io/VOSONDash/))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Abgesehen von der Version der *Twitter* API, für die sie genutzt werden können, unterscheiden sich die `R`-Pakete auch im Hinblick auf ihre Funktionalitäten sowie die Art der Daten, die sie erfassen bzw. generieren können. Die in der Forschung mit Social-Media-Daten am häufigsten verwendeten Datentypen sind Netzwerk- oder Textdaten. Beim Paket [`vosonSML`](https://vosonlab.github.io/vosonSML/) liegt der Fokus z.B. auf Netzwerkdaten; ebenso wie beim Paket [`voson.tcn`](https://vosonlab.github.io/voson.tcn/) (für die Twitter API v2), wo \"tcn\" für \"Twitter Conversation Networks\" steht. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In diesem Notebook beschränken wir uns auf die beispielhafte Sammlung einiger *Twitter*-Daten sowie deren Aufbereitung und einfaches [Text Mining](https://de.wikipedia.org/wiki/Text_Mining). Unser Fokus liegt dabei auf der (explorativen) quantitativen Textanalyse (allerdings bietet `R` mit Paketen wie [`RQDA`](https://rqda.r-forge.r-project.org/) auch Optionen für die qualitative Datenauswertung). In den folgenden Beispielen konzentrieren wir uns zudem auf die retrospektive Erfassung von Tweets (d.h. wir nutzen nicht die [Streaming-Funktion](https://developer.twitter.com/en/docs/tutorials/stream-tweets-in-real-time) der *Twitter* API für die Sammlung von Tweets in Echtzeit, welche einige der in den o.g. Übersichten genannten Tools auch unterstützen). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API-Zugang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um die Code-Beispiele in diesem Notebook ausführen zu können, wird [*Academic Research access*](https://developer.twitter.com/en/products/twitter-api/academic-research) für die [*Twitter* API v2](https://developer.twitter.com/en/docs/twitter-api) benötigt. Genauer gesagt benötigen wir ein sogenanntes [Bearer Token](https://developer.twitter.com/en/docs/authentication/oauth-2-0/bearer-tokens). Die Dokumentation für das `R`-Paket [`academictwitteR`](https://github.com/cjbarrie/academictwitteR), welches wir in diesem Notebook nutzen, enthält eine [Vignette, in der die Autorisierung für den *Twitter Academic Research Product Track* (inkl. Erstellung eines Bearer Tokens) ausführlich erklärt wird](https://cran.r-project.org/web/packages/academictwitteR/vignettes/academictwitteR-auth.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `R`-Pakete laden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bevor wir die Daten über die *Twitter* API sammeln und mit diesen arbeiten können, müssen wir die benötigten `R`-Pakete laden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "library(academictwitteR) # Sammlung der Daten\n",
    "library(tidyverse) # Aufbereitung der Daten\n",
    "library(quanteda) # Text Mining\n",
    "library(quanteda.textstats) # Textstatistiken\n",
    "library(quanteda.textplots) # Visualisierung der Textdaten"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weitere Vorbereitungsschritte"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um über das Paket `academictwitteR` Daten über die *Twitter*-API zu sammeln, müssen wir das Bearer Token nutzen, das wir zuvor generiert haben sollten."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hier ihr zuvor für den Twitter API v2 Academic Research Access generiertes Bearer Token einfügen\n",
    "bearer_token <- \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten sammeln"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die Beispiele in diesem Notebook sammeln wir alle Tweets von den Accounts von [GESIS - Leibniz-Institut für Sozialwissenschaften](https://twitter.com/gesis_org) und dem [Center for Advanced Internet Studies (CAIS)](https://twitter.com/CAISnrw) für den Zeitraum vom 01. Januar 2021 bis zum 12. Mai 2022. Die daraus resultierenden [`JSON` Files](https://de.wikipedia.org/wiki/JavaScript_Object_Notation) speichern wir im Ordner `data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_df <- get_all_tweets(\n",
    "    users = c(\"gesis_org\", \"CAISnrw\"),\n",
    "    start_tweets = \"2021-01-01T00:00:00Z\",\n",
    "    end_tweets = \"2022-05-12T00:00:00Z\",\n",
    "    n = 10000,\n",
    "    data_path = \"./data\",\n",
    "    bearer_token = bearer_token\n",
    "  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die weitere Arbeit mit den Daten erstellen wir nun aus den `JSON` Files einen `R` Dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets <- bind_tweets(data_path = \"data/\",\n",
    "                      output_format = \"tidy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir uns einen ersten Eindruck von den Daten machen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse(tweets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine Information, die für uns relevant sein könnte, ist, wie sich die Tweets über die beiden Accounts verteilen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(tweets$user_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zusätzlich zu den Tweets wollen wir auch noch Daten zu den Profilen der betrachteten Accounts sammeln."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "profiles <- get_user_profile(unique(tweets$author_id),\n",
    "                             bearer_token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch diese können wir uns nun ansehen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glimpse(profiles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten explorieren"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweets mit den meisten Likes und Retweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im ersten Schritt schauen wir uns an, welche Tweets die meisten Likes und Retweets bekommen haben. Dies machen wir jeweils für die beiden Accounts sowie Likes und Retweets getrennt und beschränken uns dabei auf Tweets, die selbst keine Retweets (oder Quotes) sind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets %>% \n",
    "  filter(user_username == \"gesis_org\",\n",
    "         is.na(sourcetweet_type)) %>% \n",
    "  arrange(-like_count) %>% \n",
    "  select(text, created_at, like_count) %>% \n",
    "  head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets %>% \n",
    "  filter(user_username == \"gesis_org\",\n",
    "         is.na(sourcetweet_type)) %>% \n",
    "  arrange(-retweet_count) %>% \n",
    "  select(text, created_at, retweet_count) %>% \n",
    "  head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets %>% \n",
    "  filter(user_username == \"CAISnrw\",\n",
    "         is.na(sourcetweet_type)) %>% \n",
    "  arrange(-like_count) %>% \n",
    "  select(text, created_at, like_count) %>% \n",
    "  head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets %>% \n",
    "  filter(user_username == \"CAISnrw\",\n",
    "         is.na(sourcetweet_type)) %>% \n",
    "  arrange(-retweet_count) %>% \n",
    "  select(text, created_at, retweet_count) %>% \n",
    "  head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Anteil Retweets und Sprachen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Als Ergänzung zur vorherigen Analyse können wir uns auch ausgeben lassen, wie hoch der Anteil Retweets, Quotes und Original-Tweets pro Account ist. Hierzu erstellen wir separate Dataframes für die beiden Accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_gesis <- tweets %>% \n",
    "  filter(user_username == \"gesis_org\")\n",
    "\n",
    "prop.table(table(tweets_gesis$sourcetweet_type, useNA = \"always\"))*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_cais <- tweets %>% \n",
    "  filter(user_username == \"CAISnrw\")\n",
    "\n",
    "prop.table(table(tweets_cais$sourcetweet_type, useNA = \"always\"))*100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eine weitere Information, die uns die *Twitter* API ausgibt, ist die für den Tweet erkannte Sprache. Auch hier können wir uns anschauen, wie die Verteilung für die beiden Accounts aussieht."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(tweets_gesis$lang)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(tweets_cais$lang)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Hinweis*: Die Spracherkennung bei Twitter funktioniert zumeist recht gut (zumindest für Englisch und Deutsch). Es ist jedoch möglich und in manchen Fällen auch sinnvoll, diese durch andere `R`-Pakete wie [`cld3`](https://docs.ropensci.org/cld3/) oder [`franc`](https://github.com/gaborcsardi/franc) zu überprüfen bzw. ergänzen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nach einer ersten groben Exploration der gesamten Daten wenden wir uns nun der Deskription der Textdaten zu. Bevor wir dies machen können, ist zunächst ein wenig Datenaufbereitung vonnöten (bei Textdaten spricht man hier häufig auch von Preprocessing). Wir beschränken uns auf Tweets in englischer Sprache, reduzieren die Zahl der Variablen und bereinigen die Texte zudem um ein paar `HTML`-Elemente (&lt und &gt stehen für < und > und &amp für &)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "remove_html <- \"&amp;|&lt;|&gt;\"\n",
    "\n",
    "tweets_en <- tweets %>% \n",
    "  filter(lang == \"en\",\n",
    "         is.na(sourcetweet_type)) %>% \n",
    "  select(tweet_id, text, user_username) %>% \n",
    "  mutate(text = str_remove_all(text, remove_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Auch für diesen reduzierten Datensatz können wir uns nochmal die Verteilung der Tweets auf die beiden Accounts anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table(tweets_en$user_username)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im nächsten Schritt erstellen wir aus dem Dateframe einen sogenannten [Corpus](https://tutorials.quanteda.io/basic-operations/corpus/corpus/), d.h. eine Sammlung von Dokumenten (in unserem Fall Tweets) plus Metadaten zu den Dokumenten (Tweets)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en_corpus <- corpus(tweets_en,\n",
    "                           docid_field = \"tweet_id\",\n",
    "                           text_field = \"text\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im folgenden Aufbereitungsschritt teilen wir die Tweets in einzelne Wörter auf. Dies nennt man [Tokenization](https://en.wikipedia.org/wiki/Lexical_analysis#Tokenization). Zudem entfernen wir bestimmte Zeichentypen und -folgen sowie sogenannte [Stoppwörter](https://de.wikipedia.org/wiki/Stoppwort) (d.h., Wörter, die sehr häufig in Texten vorkommen, für deren Inhalt jedoch i.d.R. nicht besonders wichtig sind)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en_tokens <- tokens(tweets_en_corpus,\n",
    "                           remove_punct = TRUE,\n",
    "                           remove_numbers = TRUE,\n",
    "                           remove_symbols = TRUE,\n",
    "                           remove_url = TRUE) %>% \n",
    "  tokens_tolower() %>% \n",
    "  tokens_remove(stopwords(\"english\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zuletzt erstellen wir nun eine sogenannte [Document-Feature Matrix](https://quanteda.io/reference/dfm.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en_dfm <- dfm(tweets_en_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Selbstverständlich könnte das Preprocessing der Textdaten auch anders ausfallen oder erweitert werden (z.B. durch weitere Bereinigungsschritte mithilfe von Paketen wie [`qdapRegex`](https://github.com/trinker/qdapRegex) oder [`textclean`](https://github.com/trinker/textclean)). Wichtig ist hier allerdings zu beachten, dass die Wahl der Bereinigungsschritte sowie ihre Reihenfolge wesentlich bestimmen, wie die Daten, mit denen am Ende gearbeitet wird, aussehen bzw. was sie beinhalten."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Top Hashtags & Mentions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die zuvor erstellte Document-Feature Matrix (DFM) können wir nun z.B. nutzen, um uns die häufigsten Hashtags und @-Mentions in unseren Tweets anzuschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tag_dfm <- dfm_select(tweets_en_dfm, pattern = \"#*\")\n",
    "toptag <- names(topfeatures(tag_dfm, 50))\n",
    "head(toptag, 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_dfm <- dfm_select(tweets_en_dfm, pattern = \"@*\")\n",
    "topuser <- names(topfeatures(user_dfm, 50))\n",
    "head(topuser, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Worthäufigkeiten und Vergleiche"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für die weiteren Analysen möchten wir Hashtags und @-Mentions ausschließen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en_clean <- tweets_en_dfm %>% \n",
    "  dfm_remove(pattern = \"@*\") %>% \n",
    "  dfm_remove(pattern = \"#*\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nun können wir uns beispielsweise die absoluten Worthäufigkeiten anschauen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "term_freq_en <- textstat_frequency(tweets_en_clean)\n",
    "head(term_freq_en, n = 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Allseits beliebt als anschauliches Visualisierungstool (wenn auch ungeeignet für belastbare quantitative Textanalysen) sind Wortwolken (Wordclouds)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textplot_wordcloud(tweets_en_clean, max_words = 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Etwas interessanter wird die Visualisierung, wenn wir diese nutzen, um die Inhalte der Tweets der beiden betrachteten Accounts zu vergleichen. Hierfür müssen wir zunächst eine gruppierte DFM erstellen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_en_grouped <- dfm_group(tweets_en_clean, groups = user_username)\n",
    "\n",
    "textplot_wordcloud(tweets_en_grouped, \n",
    "                   max_words = 50, \n",
    "                   comparison = TRUE, \n",
    "                   color = c(\"#009DD2\", \"#FF610E\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um den Vergleich etwas wissenschaftlicher zu machen, nutzen wir als finalen Schritt in diesem Beispielnotebook die sogenannte [Keyness](https://tutorials.quanteda.io/statistical-analysis/keyness/). Diese zeigt an, wie distinkt bestimmte Begriffe für einen Text bzw. eine Gruppe von Texten im Vergleich zu allen anderen Texten in einem Corpus sind. In unserem Fall vergleichen wir hier konkret die Tweets von *GESIS* mit denjenigen vom *CAIS*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "textstat_keyness(tweets_en_grouped, target = \"gesis_org\") %>% \n",
    "  textplot_keyness(n = 10, color = c(\"#FF610E\", \"#009DD2\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Daten speichern"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wenn wir die gesammelten Daten für die weitere Auswertung (und/oder zur Archivierung, s.u.) speichern möchten, ist dies mit `R` in vielen verschiedenen Formaten möglich. Aufgrund seiner Interoperabilität nutzen wie hier im Beispiel das CSV-Format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_csv(tweets, file = \".data/tweets.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Where to go from here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die Analysen in diesem Notebook sind nur als Beispiele gemeint. Weitere Anregungen für Analysen mit *Twitter*-Daten gibt es z.B. im Tutorial [*21 Recipes for Mining Twitter Data with rtweet*](https://rud.is/books/21-recipes/) von Bob Rudis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ausführlichere Einführungen in die quantitative Textanalyse (auch mit *Twitter*-Beispielen) gibt es u.a. hier:\n",
    "- [Text Mining with R - A Tidy Approach](https://www.tidytextmining.com/) von Julia Silge und David Robinson\n",
    "- [Text mining in R for the social sciences and digital humanities](https://tm4ss.github.io/docs/) von Andreas Niekler und Gregor Wiedemann\n",
    "- [Automatisierte Inhaltsanalyse mit R](http://inhaltsanalyse-mit-r.de/index.html) von Cornelius Puschmann"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zur Arbeit mit Social-Media-Daten bietet *GESIS* regelmäßig unterschiedliche Workshops an, wie z.B.:\n",
    "- [Linking Twitter & Survey Data](https://training.gesis.org/?site=pDetails&child=full&pID=0x43BF9C02B2F144178FBC82E82ECB5961), 27.06.2022, online\n",
    "- [Automated Web Data Collection with R](https://training.gesis.org/?site=pDetails&child=full&pID=0xB75F899267F14C648AED7D43EBFF3BFB&subID=0x44757916B99049C88889E388D33CF4EE), 12. - 16.09.2022, Mannheim\n",
    "- [Automated Web Data Collection with Python](https://training.gesis.org/?site=pDetails&child=full&pID=0xB75F899267F14C648AED7D43EBFF3BFB&subID=0xCE318FE52487447889F1ED408284EE3E), 12. - 16.09.2022, Mannheim\n",
    "\n",
    "Zudem gibt es im Programm von [*GESIS Training*](https://www.gesis.org/en/gesis-training/home) auch regelmäßig verschiedene Angebote zur Einführung in `R`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ausblick"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wie bereits eingangs erwähnt, kann man *Twitter*-Daten nicht nur über die API sammeln. Es gibt auch andere Zugangswege (siehe [Breuer et al., 2020](https://doi.org/10.1177/1461444820924622)). Dazu gehören die Nutzung bereits erhobener Daten aus fortlaufenden (z.B. [*TweetsKB*](https://data.gesis.org/tweetskb/), siehe auch [Fafalios et al., 2018](https://doi.org/10.5281/zenodo.573852)) oder abgeschlossenen Sammlungen (z.B. [*Geotagged Twitter posts from the United States*](https://doi.org/10.7802/1166), siehe auch [Kinder-Kurlanda et al., 2017](https://doi.org/10.1177/2053951717736336)). Eine weitere Option ist die Datenspende (data donation) durch Nutzende (siehe z.B. [Boeschoten et al., 2020](https://arxiv.org/abs/2011.09851)). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Natürlich umfasst die Forschung mit *Twitter*- und anderen Social-Media-Daten mehr als nur die Erhebung, Aufbereitung und Auswertung. Im gesamten [Forschungsdatenzyklus](https://www.gesis.org/angebot) ergeben sich für die Forschenden Fragen und Herausforderungen, etwa im Hinblick auf mögliche Verzerrungen (Bias) in den Daten (siehe [Sen et al., 2021](https://dx.doi.org/10.1093/poq/nfab018)), [ethische Aspekte](https://zenodo.org/record/5888912) oder auch die [Archivierung der Daten](https://zenodo.org/record/5041072). Während im Bereich der Sammlung, Aufbereitung und Auswertung solcher Daten viel passiert und Tools und Prozesse sich fortlaufend weiterentwickeln, sind etwa bezüglich der Forschunsethik und des rechtlichen Rahmens noch viele Fragen offen bzw. werden intensiv diskutiert. Und auch für die Archivierung werden derzeit (u.a. [bei *GESIS*](https://www.gesis.org/en/services/finding-and-accessing-data/collecting-digital-behavioral-data)) Lösungen entwickelt und erprobt, die sich dann noch etablieren müssen."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Und zu guter Letzt bleibt die Frage, wie lange und für wen *Twitter* relevant bleibt. Wir sehen dies sowohl in der jüngeren Medienhistorie (Stichwort *MySpace* oder *StudiVZ*) ebenso wie in der aktuellen Debatte um die Übernahmepläne von Elon Musk und die plötzlich gestiegene Popularität von *Twitter*-Alternativen wie *Mastodon* (if you want to hop on the bandwagon, join [fediscience](https://fediscience.org/about) or [scholar.social](https://scholar.social/about))."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
